\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{titling,lipsum}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{float}

\addbibresource{bibliography.bib}

\title{Grafy i sieci - wykrywanie społeczności\\dokumentacja końcowa}
\date{\today}
\author{Ireneusz Stanicki, Mateusz Śliwakowski,\\Bartłomiej Truszkowski, Przemysław Woźniakowski}

\begin{document}
	\begin{titlingpage}
		\maketitle
	\end{titlingpage}
	\pagenumbering{arabic}

\tableofcontents
\newpage

%-----------------------------------------------------%
%Wstęp
%-----------------------------------------------------%
\section{Wstęp}

%-----------------------------------------------------%
%Algorytmy
%-----------------------------------------------------%
\section{Algorytmy}

%-----------------------------------------------------%
%Girvan Newman
%-----------------------------------------------------%
\subsection{Algorytm Girvan-Newmana}
\subsubsection{Implementacja}

W przypadku tego algorytmu praca została rozpoczęta od podstaw, a więc tradycyjnej wersji algorytmu Girvan-Newman dla społeczności rozłącznych. Jak się okazało biblioteka igraph, z której korzystaliśmy, udostępnia wbudowaną funkcję do wyliczania parametru edge betweenness. Dzięki skorzystaniu z tej metody implementacja algorytmu sprowadziła się do krótkiej funkcji, która wyznacza ww. parametr, a następnie usuwa krawędź, dla której jest on największy. Algorytm ten nie podlegał dalszej analizie - wykorzystano go tylko do sprawdzenia poprawności danych, na których pracowaliśmy, a także określenia struktury wyniku użytej w całym algorytmu.\\

Aby uzyskać wynik w postaci społeczności nachodzących użyty został algorytm CONGA, do którego zastosowano własne modyfikacje. Także w tym wypadku pomocna okazała się biblioteka igraph, w której zaimplementowana jest funkcja do wyznaczania parametru vertex betweenness. Dzięki temu w każdym kroku uzyskano zbiory dwóch kluczowych parametrów dla przebiegu całego algorytmu.\\

Warto przybliżyć również strukturę wyniku, który był zwracany przez zmodyfikowany algorytm. Funkcja zwracała tablicę wartości modularnośći dla każdego z kroku algorytmu. Ponadto uzupełniana była także tablica tablic results, która zawierała wyjściowy dendrogram. Po wykonaniu kroku algorytmu do results doklejana była kolejna tablica, zawierającą krawędzie usunięte w danym kroku. W przypadku konstrukcji tej tablicy przyjęto następującą zasadę: gdy dochodziło do kopiowania wierzchołka pierwszą krawędź stanowiła pętle dla wierzchołka kopiowanego.\\

Chcąc odtworzyć rozwiązanie dla danego kroku z dendrogramu wyjściowego, konieczne było użycie tablicy Overlap, która również była uzupełniana w trakcie działania programu. Dla oryginalnych wierzchołków wartość w tej tablicy wynosiła -1, dla dodawanych wskazywała na numer wierzchołka, z którego został on skopiowany.\\

Wynik działania funkcji musiał jeszcze zostać zmodyfikowany w celu uzyskania najlepszego wyniku dla badanego grafu. Dzięki temu opierając się na współczynniku modularności określana była wartość docelowa dla danego grafu (więcej na temat w kolejnej sekcji). Następnie z wyjściowego dendrogramu graf był odtwarzany do kroku, któremu odpowiadał użyty parametr modularności. Kolejną część stanowiła modyfikacja takiego grafu do wizulizacji - konieczne było tutaj usunięcie kopii wierzchołków i ponowne złączenie ich z oryginalnymi, zachowując odpowiednie numery społeczności.

\subsubsection{Podjęte decyzje}
Pierwszą decyzję, jaką należało podjąć przy implementacji algorytmu, był wybór sposobu obliczania modularności. Miara ta jest jasno określona przez autorów oryginalnego algorytmu Girvan-Newman, lecz przy modyfikacjach dla społeczności nachodzących nie dysponujemy tego typu wzorem. Wobec tego jako miara modularności został wybrany tutaj wskaźnik użyty w przypadku algorytmu OCDLCE, który został odpowiednio dostosowany do algorytmu CONGA. Miara ta skupiona jest na jednej społeczności lokalnej, natomiast w opisywanym przypadku należało ją określić dla całego grafu. Wobec czego była ona wyliczana dla każdej uzyskanej społeczności, a następnie aby określić modularność dla całego grafu, posłożono się średnią arytmetyczną z uzyskanych wartości. Warto nadmienić, iż w postaci społeczności bez krawędzi wychodzących poza nią modularność z OCDLCE powinna zwrócić nieskończoność - w celach implementacji sprowadziliśmy ten parametr do wartości liczby wszystkich wierzchołków.\\

Kolejny ważny wybór dotyczący modularności opierał się na wyborze tej wartości, dla której uzyskany wynik jest najlepszy. Założenie początkowe miary z OCDLCE opierało się na tym, iż rezultat jest najlepszy wtedy, gdy największa jest wartość tego parametru. Niestety w takim przypadku wynik najlepszy zawsze osiągany byłby na początku, gdyż wszystkie krawędzie są wtedy wewnątrz społeczności (która de facto jest wtedy łączną składową grafu). Wobec tego także tutaj trzeba było zastosować pewne modyfikacje tej miary. Zdecydowano się na wybór maksimów lokalnych dla całej tablicy wartości modularności. Następnie z wybranych wartości dokonywano wyboru wartości wynikowej na podstawie określonego na wejściu kwartylu. Tą wartością można już było manipulować - im mniejsza jej wartość tym bardziej wynikowa społeczność była rozrzedzona. I analogicznie - chcąc otrzymać podział na większe grupy należało zwiększyć tę wartość.\\

Manipulację wielkością społeczności, jaką chcemy otrzymać, można było także uskutecznić przy pomocy preprocessingu. Polegał on na przejściu całego grafu przez algorytm, z tą różnicą, że jedyną zwracaną wartością była różnica pomiędzy wartościami edge i vertex betweenness. Bazując na wartościach z każdego kroku można było określić, jaki stopień rozdrobnienia społeczności chcemy uzyskać. W ten sposób do głównego wykonania programu można było przesłać parametr gap, który określał handicap, jaki dajemy usuwaniu krawędzi nad kopiowaniem wierzchołka. Im większą wartość dla tego parametru wybraliśmy z wyniku preprocessingu, tym społeczności wynikowe były liczniejsze (co jednak zależało także od wyboru końcowego, wspomnianego w poprzednim akapicie).\\

Przechodząc do esencji algorytmu należy wspomnieć o dokonanych własnych modyfikacjach. Początkowo założono, iż dla tego algorytmu modyfikacje będziemy stosować w procesie kopiowania wierzchołka. Zgodnie z tymi założeniami zaimplementowane zostały dwa rozwiązania. Pierwsze z nich stosuje podział wierzchołka wyłącznie na podstawie daty zawarcia znajomości. W ten sposób wyliczamy średnią z dat zawarcia znajomości dla każdej z incydentnych krawędzi danego wierzchołka, a następnie te z wartościami poniżej średniej 'przepinamy' do kopii wierzchołka, pozostawiając pozostałe z oryginałem.\\

Podobnie postępujemy w przypadku modyfikacji używającej liczby wspólnych znajomych. W tym wypadku zmieniony został jednak sposób wyliczania wartości, która stanowi odniesienie porównania przy dzieleniu. Dla danego wierzchołka wyliczana jest tutaj średnia ważona, gdzie wagę stanowi liczba wspólnych znajomych. O ile pierwszy sposób wyliczania powinien premiować prosty podział chronologiczny, tak w tym wypadku większy nacisk powinien zostać położony na bardziej zżyte ze sobą społeczności (osoby, które niekoniecznie poznały się w danym czasie, ale dłużej znajdowały się wspólnie w danej grupie).

%-----------------------------------------------------%
%LPA
%-----------------------------------------------------%
\subsection{Label Propagation Algorithm}
\subsubsection{Implementacja}

Wersja podstawowa algorytmu Label Propagation była już zaimplementowana w bibliotece igraph. Pozostało jedynie dostosowanie jej wyniku tak, aby dało się go zwizualizować za pomocą naszej warstwy prezentacji. Jako że dalsze modyfikacje opierały się na tej wersji, konieczne okazało się również zaimplementowanie jej w naszym projekcie od podstaw.\\

Posiadając już podstawową wersję algorytmu pozostało zmodyfikować ją tak, aby była ona w stanie wykrywać społeczności nachodzące. Zgodnie z dokumentacją początkową wprowadzono parametr, określający maksymalną liczbę społeczności, do której może przynależeć wierzchołek i zmodyfikowano krok algorytmu tak, aby był w stanie przypisać rozpatrzanemu wierzchołkowi więcej niż jedną społeczność.\\

W ramach tego opracowania nie udało się wprowadzić modyfikacji z parametrem znormalizowanym. Okazało się jednak, że kroki z innych algorytmów można połączyć z algorytmem LPA. Tak więc wykorzystano krok pierwszy algorytmu OCDLCE do inicjalizacji początkowych społeczności algorytmu LPA. Korzystając z faktu, że algorytm LPA jest niedeterministyczny, można było również użyć ostatniego kroku algorytmu Louvein do połączenia kilku rozwiązań rozłącznych w takie, gdzie społeczności są nachodzące.\\

Modyfikacje związane z dodatkowymi własnościami grafu zostały zrealizowane zgodnie z założeniami. Dla dat zawarcia znajomości algorytm opiera się na odchyleniu standardowym - wybiera społeczność, gdzie jest ono najmniejsze. Przy liczbie wspólnych znajomych, czy innym parametrze ilościowym (np. liczba commitów w pracy nad wspólnym repozytorium), wybierana jest grupa, gdzie suma dla tego parametru jest maksymalna.

\subsubsection{Podjęte decyzje}

W podstawowej wersji algorytmu dużym problemem była oscylacja w grafach bliskich dwudzielnym. Aby zapobiec temu problemowi wprowadzono zabezpieczenie - jeżeli obecna społeczność wierzchołka znajduje się w zbiorze społeczności, między którymi rozpatrujemy remis, to wybieramy ją jako zwycięską.\\

Kolejnym problemem, okazało się (zgodnie z oczekiwaniami) formowanie społeczności gigantycznych. Szczególnie brak zabezpieczeń tego typu był widoczny w implementacji wbudowanej w bibliotekę igraph, gdzie nawet dla niezbyt gęstych grafów algorytm często umieszczał wszystkie wierzchołki w jednej społeczności. Problem ten nie został rozwiązany wprost w naszej implementacji, ponieważ umożliwiono określanie maksymalnej liczby iteracji, co skutecznie pozwala na ograniczenie maksymalnej wielkości społeczności.\\

Zaimplementowano prostą wersję algorytmu używającej technologi CUDA. Implementacja ta powstała w celach porównawczych pod kątem czasu wykonania, zatem nie były w niej wprowadzane żadne modyfikacje. Ze względu na architekturę konieczne było również ograniczenie liczby przetwarzanych społeczności dla danego wierzchołka. Aby ograniczyć użycie pamięci lokalnej wątków zdecydowano się na rozpatrywanie maksymalnie 64 etykiet dla pojedynczego wierzchołka. W implementacji tej skupiono się wyłącznie na fazie obliczeń. Przekształcanie reprezentacji grafu zaimplementowano naiwnie, jako że nie jest ono istotna w rozważaniach.

%-----------------------------------------------------%
%OCDLCE
%-----------------------------------------------------%
\subsection{Overlapping Community Detection by Local Community Expansion}
\subsubsection{Implementacja}
\subsubsection{Podjęte decyzje}

%-----------------------------------------------------%
%Louvain
%-----------------------------------------------------%
\subsection{Algorytm Louvain}

\subsubsection{Implementacja}
Algorytm Louvain został zaimplementowany od podstaw. Pierwszym krokiem było dodanie wszystkich funkcji pomocniczych, koniecznych do obliczenia wartości takich jak liczba połączeń do społeczności (nie wliczając pętli) czyli $\Sigma_{tot}$, liczba połączeń wewnątrz społeczności czyli $\Sigma_{in}$, czy liczba połączeń do wierzchołka k rozpatrywanej społeczności $k_{in}$. Funkcje te wywoływane są tylko w koniecznych sytuacjach. W miarę możliwości odpowiednie parametry są uaktualniane, zamiast ciągłego ich przeliczania.\\ \\
Podstawą algorytmu jest funkcja $louvain\_ iteration$, która odpowiada pojedynczej iteracji algorytmu. Jej zadaniem jest uaktualnianie tablicy przynależności, która mówi w jakiej społeczności aktualnie znajduje się wierzchołek.\\ Na początku wyznaczana jest w niej suma wag krawędzi w grafie i tworzona jest kopia tablicy przynależności, a następnie za pomocą pętli iterujemy po wszystkich wierzchołkach. Kolejność rozpatrywania wierzchołków zależy od tablicy $permut$, która jest losową permutacją liczb od $1$ do $n$ ($n$-liczba wierzchołków). \\Dla każdego wierzchołka rozpatrywane są społeczności do których należą jego sąsiedzi (każda społeczność jest brana pod uwagę tylko raz). Wierzchołek jest usuwany ze swojej społeczności i umieszczany w społeczności sąsiada, a następnie obliczany jest przyrost modularności. Jeżeli przyrost dla społeczności jest większy od aktualnego maksymalnego przyrostu ($max\_ mod\_ change$) to staje się on nowym maksymalnym przyrostem, a społeczność ta jest zapamiętywana. Następnie wierzchołek wraca do swojej społeczności i proces jest powtarzany dla każdego sąsiada. \\
Warto tu przypomnieć wzór na zmianę modularności: $$\Delta Q = [\frac{\Sigma_{in} +2k_{i,in}}{2m} - (\frac{\Sigma_{tot} + k_i}{2m})^2] - [\frac{\Sigma_{in}}{2m} - (\frac{\Sigma_{tot}}{2m})^2 - (\frac{k_i}{2m})^2]$$ gdzie $k_i$ to stopień wierzchołka $i$, $k_{in}$ to suma wag krawędzi między wierzchołkiem $i$ i wierzchołkami ze społeczności, do której trafia $i$, $\Sigma_{in}$ to suma wag krawędzi wewnątrz tej społeczności, a $\Sigma_{tot}$ to suma wag krawędzi wewnątrz społeczności z której usuwamy wierzchołek $i$.\\Jest to jednak rozpisany wzór i w implementacji wykorzystana jest jego skrócona wersja: $$\Delta Q = [\frac{k_{i,in}}{m} - \frac{\Sigma_{tot}*k_i}{2m^2}]$$ \\
\\
Po przeiterowaniu wszystkich sąsiadów wierzchołka, sprawdzane jest czy znaleziono społeczność zwiększającą modularność, która nie jest oryginalną społecznością wierzchołka. Jeżeli tak jest, to odnotowywana jest zmiana, oraz uaktualniane są odpowiednie wartości i tablica przynależności (zarówno dla danego wierzchołka, jak i dla wszystkich, które zostały z nim scalone).
\\
Po przejściu wszystkich wierzchołków, tablica przynależności jest podmieniana i zwracana jest wartość logiczna, mówiąca czy nastąpiła zmiana.  
\\
Równie ważna funkcją w algorytmie jest funkcja scalająca wierzchołki znajdujące się w jednej społeczności - $merge\_ communities$. Tworzy ona nowy graf, w którym każdej społeczności odpowiada wierzchołek umieszczony w tej społeczności. Suma wag krawędzi między wierzchołkami wewnątrz społeczności staje się pętlą w nowym grafie, a suma krawędzi między społecznościami wagą krawędzi pomiędzy odpowiadającymi im wierzchołkami.
\\ \\
Cały algorytm wykonywany jest w funkcji $louvain\_ algorithm$. Wyznaczana jest w niej wartość $\Sigma_{tot}$ dla każdej społeczności, tworzona jest kopia grafu oraz każdy wierzchołek umieszczany jest w swojej społeczności. Następnie w pętli wykonywana jest funkcja $louvain\_ iteration$, tak długo jak powoduje ona jakieś zmiany społeczności. Po każdej iteracji społeczności są scalane. Wynikiem algorytmu jest tablica przynależności. Algorytm nie modyfikuje oryginalnego grafu.

\subsubsection{Implementacja dla nachodzących społeczności}
Algorytm dla społeczności nachodzących wykorzystuje algorytm dla społeczności rozłącznych. Został przede wszystkim zaimplementowany do współpracy z algorytmem Louvaina, ale może działać też z innym nie deterministycznymi algorytmami. Muszą one jednak zwracać tablicę przynależności. \\
Celem algorytmu jest skonstruowanie macierzy przynależności. Jest to macierz $n \times k$, gdzie $n$ to ilość wierzchołków, a $k$ to ilość społeczności. Element $[i][j]$ oznacza jak bardzo wierzchołek i należy do społeczności j.
\\ Algorytm rozpoczyna się od pojedynczego przebiegu algorytmu dla społeczności rozłącznych. Jego wynikowa tablica jest tłumaczona tak aby numery społeczności były liczbami od $0$ do $k-1$ ($k$ -ilość społeczności w wyznaczonym podziale). Będzie podstawą do wygenerowania macierzy przynależności. Jest ona od razu aktualizowana. Do elementu $[i][j]$ dodajemy ilość połączeń wierzchołka $i$ z wierzchołkami ze społeczności $j$.\\
Następnie wywołujemy algorytm dla społeczności rozłącznych określoną ilość razy. Po każdym przebiegu wynikowa tablica jest tłumaczona, tak by społeczności odpowiadające tym w tablicy z oryginalnego przebiegu miały ten sam numer. W tym celu dla każdej społeczności znajdowana jest taka, w której ilość pokrywających się wierzchołków jest największa. Następnie macierz przynależności jest aktualizowana, analogicznie jak po początkowym przebiegu.
\\ Po wykonaniu założonych przebiegów, macierz przynależności jest normalizowana (tak by suma współczynników dla każdego wierzchołka wynosiła 1). Następnie każdemu wierzchołkowi przydzielane są społeczności, dla których współczynnik przynależności przekroczył wartość progową (obliczaną na podstawie parametru funkcji). Współczynniki dla społeczności, które nie przekroczyły wartości progowej są zerowane, a macierz znów jest normalizowana. Jeżeli wierzchołek dla wszystkich społeczności ma współczynnik 0 (tzn. jest wyizolowany), to jest on przydzielany do swojej społeczności. Jeżeli dla wierzchołka, żaden ze współczynników nie przekroczy wartości progowej, to umieszczany jest on w społeczności z największym współczynnikiem. Jest to rozwiązanie na swój sposób prowizoryczne. Takie wierzchołki mogą być bowiem czasami łącznikami pomiędzy społecznościami i w takich sytuacjach wypadałoby umieścić je we wszystkich społecznościach. Jest to jednak problem, który nie został poruszony w tej implementacji i może być obiektem usprawnień w przyszłości.
\subsubsection{Podjęte decyzje}
Metoda dla społeczności nachodzących opisana w artykule "A Fast Algorithm for Overlapping Community Detection" \cite{pw-paper3}, zakładała obecność jeszcze jednego kroku, którego celem miała być ogólna poprawa jakości wyniku. Metoda ta opierała się jednak na modularności Nicosii \cite{pw-paper2}, której obliczenie jest na tyle kosztowne, że zwiększa złożoność algorytmu do $n^3$. Z racji, że zdecydowanie pogorszyło by to czas obliczeń i negatywnie wpłynęło na możliwość wielokrotnego testowania (zwłaszcza dla grafów o kilku tysiącach wierzchołków), a także biorąc pod uwagę fakt, że wyniki bez tego kroku były zadowalające, postanowiliśmy z niego zrezygnować.\\\ Dodatkową modyfikacją jest możliwość przekazywania  opcjonalnego parametru do wywołania funkcji $louvain\_ algorithm$. Parametr ten wpływa na to jak bardzo element $\frac{\Sigma_{tot}*k_i}{2m^2}$ wpływa na zmianę modularności. Dla wartości mniejszych promuje on powstawanie niewielu, rozległych społeczności, a dla większych - powstawanie wielu drobniejszych społeczności. Najlepsze wartości modularności uzyskiwane są co prawda dla wartości 1, ale parametr pozwala na dostosowywanie wyniku do naszych oczekiwań, przy nieznacznych stratach na jakości.

%-----------------------------------------------------%
%Dane testowe
%-----------------------------------------------------%
\section{Dane testowe}
Rozpoczynając pracę, zakładaliśmy, że opisane algorytmy będziemy badać na zbiorach danych udostępnianych przez firmę Facebook. Portal ten chyba najbardziej kojarzy się z ogromnym źródłem danych dotyczących społeczności. Niestety modyfikacje, które chcieliśmy zastosować, nie mogły zostać zrealizowane na datasetach ściągniętych z tego portalu. Żaden z szeroko dostępnych zbiorów nie zawierał informacji na temat liczby wspólnych znajomych, a także daty zawarcia znajomości. O ile zdobycie tej pierwszej wiadomośći jest bardzo proste przy użyciu nawet najbardziej naiwnego algorytmu brute force, tak uzyskanie dat można było przeprowadzić tylko w sposób losowy, co mogło by tylko zaburzyć osiągane wyniki. Wobec tego swe spojrzenie skierowaliśmy w stronę zbiorów, które na pierwszy rzut oka nie kojarzą się ze społecznościami tak dobrze jak Facebook.
%-----------------------------------------------------%
%NBA
%-----------------------------------------------------%
\subsection{NBA}
'I loved this game' - to slogan reklamowy towarzyszący najlepszej lidze koszykarskiej, który odnosi się także do jednego z autorów tej pracy. Wybór tego zbioru danych motywowany był prywatnymi zainteresowaniami autorów, które wiążą się także z ogólną wiedzą na temat samej ligi. Dzięki temu zyskaliśmy możliwość oceny osiąganych rezultatów 'ludzkim okiem', przez co można było je poprawiać i oceniać dzięki swego rodzaju eksperckiej wiedzy na temat NBA.\\

Swoją rolę odegrała tu także ogólna dostępność danych. Ich źródło stanowił portal basketball-reference.com, który stanowi internetową, koszykarską encyklopedię. Korzystając ze scrapera napisanego w języku JavaScript można było zautomatyzować proces wyszukiwania danych. Pomógł tutaj alfabetyczny spis koszykarzy - w ten sposób zdobyliśmy informację na temat każdego z nich, a także w łatwy sposób uzyskaliśmy liczbę wszystkich sezonów i klubów, w jakich w swojej karierze występował dany zawodnik. Następnie dane te zostały obrobione z wykorzystaniem SQL, w celu uzyskania relacji między zawodnikami, a także daty zawarcia znajomości. Liczba wspólnych znajomych została określona przy pomocy prostego algorytmu napisanego w języku C\#.\\

Graf, który uzyskaliśmy, składa się z 4800 wierzchołków. Każdy wierzchołek można przypisać zawodnikowi, który postawił swą stopę na parkietach NBA w ciągu ponad 70 lat istnienia ligi. Dwaj zawodnicy są ze sobą w relacji, jeśli kiedykolwiek zagrali ze sobą w jednym klubie. Data zawarcia znajomości to w tym wypadku data pierwszego meczu, w którym obaj zawodnicy mieli okazję ze sobą zagrać. Tworząc relacje w ten sposób uzyskaliśmy ponad 146 tysięcy krawędzi.

%-----------------------------------------------------%
%Filmweb
%-----------------------------------------------------%
\subsection{Filmweb}
Poszukując parametru daty zawarcia znajomości zwróciliśmy się także ku innej części branży rozrywkowej. Źrodło danych w tym wypadku stanowił portal filmweb.pl, który dysponuje ogromną bazą danych na temat aktorów i filmów. Skala tych danych jest tak duża, iż musieliśmy ograniczyć nasze prace tylko do polskich aktorów. W ten sposób uzyskaliśmy graf z 10 000 wierzchołków i 190031 krawędziami.\\

Sposób uzyskania danych był analogiczny do tego prezentowanego w przypadku grafu NBA. Skonstruowany w języku JavaScript scraper został wykorzystany do przejrzenia listy wszystkich polskich aktorów, by następnie dla każdego z nich znaleźć listę kolegów z branży, z którymi najczęśniej występował w jednym filmie. Ten zbiór uczyniliśmy znajomymi badanego aktora. Datę zawarcia znajomości w tym wypadku stanowiła data premiery pierwszego filmu/serialu, w którym dana dwójka ze sobą wystąpiła. Dodatkowo na prośbę prowadzącego zbiór danych uzupełniony został o liczbę filmów, w których dana dwójka wspólnie występowała. Informacja ta okazała się również przydatna w modyfikacji jednego z algorytmów.\\

Podsumowując, wierzchołki uzyskanego grafu stanowią polscy aktorzy, pobrani z ogólnodostępnej bazy filmweb. Dwaj aktorzy są ze sobą w relacji, jeśli jeden z nich znajduje się w zakładce 'najczęściej występował z ...' u drugiego. Motywację wyboru tego grafu jako testowego stanowiło ogólne zainteresowanie branżą filmową autorów pracy, a także ogólna dostępność danych. Sama etykietyzacja wierzchołków pozwoliła nam łatwiej oceniać poprawność działania algorytmów - zwłaszcza w początkowej fazie implementacji z wykorzystaniem tylko małych zbiorów danych.

%-----------------------------------------------------%
%Github
%-----------------------------------------------------%
\subsection{GitHub}

%-----------------------------------------------------%
%Wyniki
%-----------------------------------------------------%
\section{Wyniki}

%-----------------------------------------------------%
%Girvan Newman
%-----------------------------------------------------%
\subsection{Algorytm Girvan-Newmana}
\subsubsection{Analiza czasowa}
\subsubsection{Analiza jakościowa}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-image-sample.png}
\caption{Przykładowe wyjście}
\end{figure}

%-----------------------------------------------------%
%LPA
%-----------------------------------------------------%
\subsection{Label Propagation Algorithm}
\subsubsection{Analiza czasowa}
Poniżej przedstawiono wyniki analizy czasowej dla standardowego zbioru przykładów użytego w pracy.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-benchmark.png}
\caption{Wynik analizy czasowej algorytmu Label Propagation}
\end{figure}

Algorytm LPA, zgodnie z oczekiwaniami, okazał się być algorytmem wystarczająco szybkim, aby przeprowadzić obliczenia na wszystkich zbiorach danych w projekcie. Z wyników można odczytać, że implementacja biblioteczna jest nieporównywalnie szybsza od implementacji własnej dla dużych zbiorów danych. Jedną z przyczyn takich wyników, może być fakt, że backend biblioteki igraph jest napisany w języku C, gdzie dostęp do elementów grafu jest dużo szybszy niż przy użyciu dodatkowej warstwy abstrakcji. Innym powodem może być nieświadome użycie w implementacji operacji, która zmusza bibliotekę do przebudowania struktury grafu. Niestety, podczas analizy kodu źródłowego rozwiązania, nie udało się znaleźć źródła problemu.\\

Na powyższym wykresie można zauważyć, że zależność czasu od ilości wierzchołków jest liniowa. Pomiarem, który zdecydowanie odbiegł od oczekiwań, jest pomiar dla największego grafu przy modyfikacji związanej z datami. Prawdopodobna przyczyna wiąże się z tym, że algorytm z modyfikacją podczas rozpatrzania remisu tworzy tablice pomocnicze oraz oblicza dla nich odchylenie standardowe. W przypadku grafu gęstego remisy występują często, co skutecznie spowolniło działanie programu.\\

Jak szybkie potrafią być algorytmy do wykrywania społeczności pokazała implementacja przy użyciu biblioteki CUDA. Rezultat znacząco przekroczył nasze oczekiwania. Co prawda implementacja ta jest częściowo okrojona, lecz z powodzeniem można ją rozszerzać o modyfikacje zaimplementowane w wersji na CPU.

%TODO: cuda plot

W podanych wynikach nie uwzględniam przebudowy reprezentacji grafu - została ona zaimplementowana naiwnie, ponieważ możemy założyć, że na wejściu otrzymujemy rządaną reprezentację. Uwzględniony został jednak czas alokacji pamięci na karcie graficznej - jest on znacząco większy od czasu obliczeń. Jego znaczenie może zostać zredukowane, gdy przetwarzamy graf kilkukrotnie, wtedy wystarczy nam jedna alokacja, na początku działania programu.

\subsubsection{Analiza jakościowa}

Analizę jakościową rozpoczniemy od porównania z implementacją wbudowaną w bibliotekę igraph.

%-----------------------------------------------------%
%OCDLCE
%-----------------------------------------------------%
\subsection{Overlapping Community Detection by Local Community Expansion}
\subsubsection{Analiza czasowa}
\subsubsection{Analiza jakościowa}

%-----------------------------------------------------%
%Louvain
%-----------------------------------------------------%
\subsection{Algorytm Louvain}
\subsubsection{Analiza czasowa}
\subsubsection{Analiza jakościowa}

%-----------------------------------------------------%
%Porównanie
%-----------------------------------------------------%
\subsection{Porównanie}

%-----------------------------------------------------%
%Wnioski
%-----------------------------------------------------%
\subsection{Wnioski}

\newpage
\printbibliography

\end{document}

