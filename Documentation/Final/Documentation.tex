\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{titling,lipsum}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{float}

\addbibresource{bibliography.bib}

\title{Grafy i sieci - wykrywanie społeczności\\dokumentacja końcowa}
\date{\today}
\author{Ireneusz Stanicki, Mateusz Śliwakowski,\\Bartłomiej Truszkowski, Przemysław Woźniakowski}

\begin{document}
	\begin{titlingpage}
		\maketitle
	\end{titlingpage}
	\pagenumbering{arabic}

\tableofcontents
\newpage

%-----------------------------------------------------%
%Wstęp
%-----------------------------------------------------%
\section{Wstęp}
Problem wykrywania społeczności w sieciach okazał się być bardzo ciekawym, a jednocześnie przystępnym zagadnieniem. W ramach poniższego opracowania przedstawiamy opis czterech zaimplementowanych przez nas algorytmów wraz z modyfikacjami, informacje na temat zebranych przez nas danych oraz wyniki działania. Postaramy się przybliżyć jakie problemy napotkaliśmy podczas pracy, czy otrzymane rezultaty są zgodne z oczekiwaniami przedstawionymi w dokumentacji wstępnej oraz jakie wnioski można wyciągnąć po przeprowadzonych eksperymentach.

%-----------------------------------------------------%
%Algorytmy
%-----------------------------------------------------%
\section{Algorytmy}

%-----------------------------------------------------%
%Girvan Newman
%-----------------------------------------------------%
\subsection{Algorytm Girvan-Newmana}
\subsubsection{Implementacja}

W przypadku tego algorytmu praca została rozpoczęta od podstaw, a więc tradycyjnej wersji algorytmu Girvan-Newman dla społeczności rozłącznych. Jak się okazało, biblioteka igraph, z której korzystaliśmy, udostępnia wbudowaną funkcję do wyliczania parametru edge betweenness. Dzięki skorzystaniu z tej metody implementacja algorytmu sprowadziła się do krótkiej funkcji, która wyznacza ww. parametr, a następnie usuwa krawędź, dla której jest on największy. Algorytm ten nie podlegał dalszej analizie - wykorzystano go tylko do sprawdzenia poprawności danych, na których pracowaliśmy, a także określenia struktury wyniku użytej w całym algorytmu.\\

Aby uzyskać wynik w postaci społeczności nachodzących użyty został algorytm CONGA, do którego zastosowano własne modyfikacje. Także w tym wypadku pomocna okazała się biblioteka igraph, w której zaimplementowana jest funkcja do wyznaczania parametru vertex betweenness. Dzięki temu w każdym kroku uzyskano zbiory dwóch kluczowych parametrów dla przebiegu całego algorytmu.\\

Warto przybliżyć również strukturę wyniku, który był zwracany przez zmodyfikowany algorytm. Funkcja zwracała tablicę wartości modularnośći dla każdego z kroku algorytmu. Ponadto uzupełniana była także tablica tablic results, która zawierała wyjściowy dendrogram. Po wykonaniu kroku algorytmu do results doklejana była kolejna tablica, zawierającą krawędzie usunięte w danym kroku. W przypadku konstrukcji tej tablicy przyjęto następującą zasadę: gdy dochodziło do kopiowania wierzchołka pierwszą krawędź stanowiła pętle dla wierzchołka kopiowanego.\\

Chcąc odtworzyć rozwiązanie dla danego kroku z dendrogramu wyjściowego, konieczne było użycie tablicy Overlap, która również była uzupełniana w trakcie działania programu. Dla oryginalnych wierzchołków wartość w tej tablicy wynosiła -1, dla dodawanych wskazywała na numer wierzchołka, z którego został on skopiowany.\\

Wynik działania funkcji musiał jeszcze zostać zmodyfikowany w celu uzyskania najlepszego wyniku dla badanego grafu. Dzięki temu opierając się na współczynniku modularności określana była wartość docelowa dla danego grafu (więcej na temat w kolejnej sekcji). Następnie z wyjściowego dendrogramu graf był odtwarzany do kroku, któremu odpowiadał użyty parametr modularności. Kolejną część stanowiła modyfikacja takiego grafu do wizulizacji - konieczne było tutaj połączenie wszystkich kopii wierzchołków, przy zachowaniu odpowiednich numerów społeczności.

\subsubsection{Podjęte decyzje}
Pierwszą decyzję, jaką należało podjąć przy implementacji algorytmu, był wybór sposobu obliczania modularności. Miara ta jest jasno określona przez autorów oryginalnego algorytmu Girvan-Newman, lecz przy modyfikacjach dla społeczności nachodzących nie dysponujemy tego typu wzorem. Wobec tego jako miara modularności został wybrany tutaj wskaźnik użyty w przypadku algorytmu OCDLCE, który został odpowiednio dostosowany do algorytmu CONGA. Miara ta skupiona jest na jednej społeczności lokalnej, natomiast w opisywanym przypadku należało ją określić dla całego grafu. Wobec czego była ona wyliczana dla każdej uzyskanej społeczności, a następnie aby określić modularność dla całego grafu, posłużono się średnią arytmetyczną z uzyskanych wartości. \\

Kolejny ważny wybór dotyczący modularności opierał się na wyborze tej wartości, dla której uzyskany wynik jest najlepszy. Założenie początkowe miary z OCDLCE opierało się na tym, iż rezultat jest najlepszy wtedy, gdy największa jest wartość tego parametru. W wypadku algorytmu CONGA również można skorzystać z takowego założenia, jednak nie można zapominać o plusach tego algorytmu, a więc łatwego odzyskiwania społeczności wynikowych w zależności od preferencji co do ich rozmiaru/kształtu. Aby to umożliwić, zdecydowano się na wybór maksimów lokalnych dla całej tablicy wartości modularności. Następnie z wybranych wartości dokonywano wyboru wartości wynikowej na podstawie określonego na wejściu kwantylu. Tą wartością można już było manipulować - im mniejsza jej wartość tym bardziej wynikowa społeczność była rozrzedzona. I analogicznie - chcąc otrzymać podział na większe grupy należało zwiększyć tę wartość.\\

Manipulację liczbą społeczności, jaką chcemy otrzymać, można było także uskutecznić przy pomocy preprocessingu. Polegał on na przejściu całego grafu przez algorytm, z tą różnicą, że jedyną zwracaną wartością była tablica różnic pomiędzy wartościami edge i vertex betweenness. Bazując na wartościach z każdego kroku można było określić, jaki stopień rozdrobnienia społeczności chcemy uzyskać. W ten sposób do głównego wykonania programu można było przesłać parametr gap, który określał priorytet, jaki dajemy usuwaniu krawędzi nad kopiowaniem wierzchołka. Im większą wartość dla tego parametru wybraliśmy z wyniku preprocessingu, tym społeczności wynikowe były liczniejsze (co jednak zależało także od wyboru końcowego, wspomnianego w poprzednim akapicie).\\

Przechodząc do esencji algorytmu należy wspomnieć o dokonanych własnych modyfikacjach. Początkowo założono, iż dla tego algorytmu modyfikacje będziemy stosować w procesie kopiowania wierzchołka. Zgodnie z tymi założeniami zaimplementowane zostały dwa rozwiązania. Pierwsze z nich stosuje podział wierzchołka wyłącznie na podstawie daty zawarcia znajomości. W ten sposób wyliczamy średnią z dat zawarcia znajomości dla każdej z incydentnych krawędzi danego wierzchołka, a następnie te z wartościami poniżej średniej 'przepinamy' do kopii wierzchołka, resztę pozostawiając z oryginałem.\\

Podobnie postępujemy w przypadku modyfikacji używającej liczby wspólnych znajomych. W tym wypadku zmieniony został jednak sposób wyliczania wartości, która stanowi odniesienie porównania przy dzieleniu. Dla danego wierzchołka wyliczana jest tutaj średnia ważona, gdzie wagę stanowi liczba wspólnych znajomych. O ile pierwszy sposób wyliczania powinien premiować prosty podział chronologiczny, tak w tym wypadku większy nacisk powinien zostać położony na bardziej zżyte ze sobą społeczności (osoby, które niekoniecznie poznały się w danym czasie, ale dłużej znajdowały się wspólnie w danej grupie).\\

Usuwanie krawędzi w trakcie działania algorytmu dokonywano zbiorowo - wszystkie krawędzie, dla których wartość parametru edge betweenness była największa, były usuwane w jednym kroku algorytmu.\\

%-----------------------------------------------------%
%LPA
%-----------------------------------------------------%
\subsection{Label Propagation Algorithm}
\subsubsection{Implementacja}

Wersja podstawowa algorytmu Label Propagation była już zaimplementowana w bibliotece igraph. Pozostało jedynie dostosowanie jej wyniku tak, aby dało się go zwizualizować za pomocą wspólnej dla projektu warstwy prezentacji. Jako że dalsze modyfikacje opierały się na tej wersji, konieczne okazało się również zaimplementowanie jej od podstaw.\\

Posiadając już bazową wersję algorytmu, pozostało zmodyfikować ją tak, aby była ona w stanie wykrywać społeczności nachodzące. Zgodnie z dokumentacją początkową wprowadzono parametr, określający maksymalną liczbę społeczności, do której może przynależeć wierzchołek i zmodyfikowano krok algorytmu tak, aby był w stanie przypisać rozpatrywanemu wierzchołkowi więcej niż jedną społeczność.\\

W ramach tego opracowania nie udało się wprowadzić modyfikacji z parametrem znormalizowanym. Okazało się jednak, że kroki z innych algorytmów można połączyć z algorytmem LPA. Tak więc wykorzystano krok pierwszy algorytmu OCDLCE do inicjalizacji początkowych społeczności algorytmu LPA. Korzystając z faktu, że algorytm LPA jest niedeterministyczny, można było również użyć ostatniego kroku algorytmu Louvain do połączenia kilku rozwiązań rozłącznych w takie, gdzie społeczności są nachodzące.\\

Modyfikacje związane z dodatkowymi własnościami grafu zostały zrealizowane zgodnie z założeniami. Dla dat zawarcia znajomości algorytm opiera się na odchyleniu standardowym - wybiera społeczność, gdzie jest ono najmniejsze. Przy liczbie wspólnych znajomych lub innym parametrze ilościowym (np. liczbie commitów w pracy nad wspólnym repozytorium), wybierana jest grupa, gdzie suma dla tego parametru jest maksymalna.

\subsubsection{Podjęte decyzje}

W podstawowej wersji algorytmu dużym problemem była oscylacja w grafach bliskich dwudzielnym. Aby zapobiec temu problemowi wprowadzono zabezpieczenie - jeżeli obecna społeczność wierzchołka znajduje się w zbiorze społeczności, między którymi rozpatrujemy remis, to wybieramy ją jako zwycięską.\\

Kolejnym problemem, okazało się (zgodnie z oczekiwaniami) formowanie społeczności gigantycznych. Szczególnie brak zabezpieczeń tego typu był widoczny w implementacji wbudowanej w bibliotekę igraph, gdzie nawet dla niezbyt gęstych grafów algorytm często umieszczał wszystkie wierzchołki w jednej społeczności. Problem ten nie został rozwiązany wprost w naszej implementacji, ponieważ umożliwiono określanie maksymalnej liczby iteracji, co skutecznie pozwala na ograniczenie maksymalnej wielkości społeczności.\\

Zaimplementowano prostą wersję algorytmu używającej technologi CUDA. Implementacja ta powstała w celach porównawczych pod kątem czasu wykonania, zatem nie były w niej wprowadzane żadne modyfikacje. Ze względu na architekturę konieczne było również ograniczenie liczby przetwarzanych społeczności dla danego wierzchołka. Aby ograniczyć użycie pamięci lokalnej wątków zdecydowano się na rozpatrywanie maksymalnie 64 etykiet dla pojedynczego wierzchołka. W implementacji tej skupiono się wyłącznie na fazie obliczeń. Przekształcanie reprezentacji grafu zaimplementowano naiwnie, jako że nie jest ono istotna w rozważaniach.

%-----------------------------------------------------%
%OCDLCE
%-----------------------------------------------------%
\subsection{Overlapping Community Detection by Local Community Expansion}
\subsubsection{Implementacja}

Algorytm OCDLCE został w pełni własnoręcznie zaimplementowany. W celu łatwiejszych modyfikacji został on podzielony na 3 funkcje, odpowiadające jego krokom. Dwa pierwsze kroki zwracają graf oraz listę społeczności, zaś trzeci jedynie końcowy graf. \\

W pierwszym kroku rozpratrywany jest po kolei każdy wierzchołek, a następnie, dla wybranego wierzchołka sprawdzani są wszyscy jego sąsiedzi, z którymi nie jest on jeszcze w jednej społeczności. W dalszej kolejności, zgodnie z założeniami algorytmu, sprawdza się wspólnych sąsiadów, których po zbadaniu modularności można dodać do tymczasowej społeczności. W przypadku gdy osiągnęła ona 5 wierzchołków, aktualizowana jest zarówno informacje w wierzchołkach o przynależności do społeczności jak i lista wszystkich społeczności.\\

W drugim kroku łączone są społeczności. Została w tym celu stworzona specjalna funkcja Union, która za pomocą wyliczenia parametru $WOS$ definiuje czy społeczności powinny zostać złączone. \\

W ostatnim kroku rozpatruje się wierzchołki nie należące do żadnej społeczności. Dla każdego takiego wierzchołka, rozpatruje się wszystkie społeczności sąsiadów, oblicza modularność i w przypadku pozytywnego wyniku, dodaje go do danej społeczności.

\subsubsection{Podjęte decyzje}
Do pierwszego i trzeciego kroku dodano możliwość podania innej miary lokalnej modularności. W wersji podstawowej użyto modularności M, opisanej szczegółowo w dokumentacji początkowej. Dla przypomnienia, wyraża się ona wzorem:
$$M = \frac{M_{in}}{M_{out}}$$
Zaimplementowana modyfikacja umożliwia zamiast tej miary skorzystanie z modularności f, opisanej wzorem:
$$f_G = \frac{k^G_{in}}{(k^G_{in} + k^G_{out})^\alpha},$$
gdzie $k^G_{in}$ jest dwukrotnością liczby krawędzi o obu końcach w G, $k^G_{out}$ liczbą krawędzi o jednym końcu w G, zaś alpha parametrem.\\


Kolejną modyfikacją była zmiana sposobu wyznaczania lokalnych społeczności w pierwszym kroku algorytmu. Zamiast każdej pary wierzchołków niebędących w społeczności, można rozpatrywać trójkę. Podejście to sprawia, że w pierwszym kroku uzyskuje się mniej początkowych społeczności, ale za to mocniej ze sobą związanych. Posiada ono jednak pewną wadę, zazwyczaj niepomijalna część wierzchołków o małej liczbie sąsiadów nie ma przypisanej żadnej społeczności.

%-----------------------------------------------------%
%Louvain
%-----------------------------------------------------%
\subsection{Algorytm Louvain}

\subsubsection{Implementacja}
Algorytm Louvain został zaimplementowany od podstaw. Pierwszym krokiem było dodanie wszystkich funkcji pomocniczych, koniecznych do obliczenia wartości takich jak liczba połączeń do społeczności (nie wliczając pętli) czyli $\Sigma_{tot}$, liczba połączeń wewnątrz społeczności czyli $\Sigma_{in}$, czy liczba połączeń do wierzchołka k rozpatrywanej społeczności $k_{in}$. Funkcje te wywoływane są tylko w koniecznych sytuacjach. W miarę możliwości odpowiednie parametry są uaktualniane, zamiast ciągłego ich przeliczania.\\

Podstawą algorytmu jest funkcja $louvain\_ iteration$, która odpowiada pojedynczej iteracji algorytmu. Jej zadaniem jest uaktualnianie tablicy przynależności, która mówi w jakiej społeczności aktualnie znajduje się wierzchołek. Na początku wyznaczana jest w niej suma wag krawędzi w grafie i tworzona jest kopia tablicy przynależności, a następnie za pomocą pętli iterujemy po wszystkich wierzchołkach. Kolejność rozpatrywania wierzchołków zależy od tablicy $permut$, która jest losową permutacją liczb od $1$ do $n$ ($n$-liczba wierzchołków).\\

Dla każdego wierzchołka rozpatrywane są społeczności do których należą jego sąsiedzi (każda społeczność jest brana pod uwagę tylko raz). Wierzchołek jest usuwany ze swojej społeczności i umieszczany w społeczności sąsiada, a następnie obliczany jest przyrost modularności. Jeżeli przyrost dla społeczności jest większy od aktualnego maksymalnego przyrostu ($max\_ mod\_ change$) to staje się on nowym maksymalnym przyrostem, a społeczność ta jest zapamiętywana. Następnie wierzchołek wraca do swojej społeczności i proces jest powtarzany dla każdego sąsiada. Warto tu przypomnieć wzór na zmianę modularności: 
$$\Delta Q = [\frac{\Sigma_{in} +2k_{i,in}}{2m} - (\frac{\Sigma_{tot} + k_i}{2m})^2] - [\frac{\Sigma_{in}}{2m} - (\frac{\Sigma_{tot}}{2m})^2 - (\frac{k_i}{2m})^2],$$
gdzie $k_i$ to stopień wierzchołka $i$, $k_{in}$ to suma wag krawędzi między wierzchołkiem $i$ i wierzchołkami ze społeczności, do której trafia $i$, $\Sigma_{in}$ to suma wag krawędzi wewnątrz tej społeczności, a $\Sigma_{tot}$ to suma wag krawędzi wewnątrz społeczności z której usuwamy wierzchołek $i$. Wzór ten można jednak skrócić do postaci, która została wykorzystana w implementacji:
$$\Delta Q = [\frac{k_{i,in}}{m} - \frac{\Sigma_{tot}*k_i}{2m^2}]$$ \\

Po przeiterowaniu wszystkich sąsiadów wierzchołka, sprawdzane jest czy znaleziono społeczność zwiększającą modularność, która nie jest oryginalną społecznością wierzchołka. Jeżeli tak jest, to odnotowywana jest zmiana, oraz uaktualniane są odpowiednie wartości i tablica przynależności (zarówno dla danego wierzchołka, jak i dla wszystkich, które zostały z nim scalone). Po przejściu wszystkich wierzchołków, tablica przynależności jest podmieniana i zwracana jest wartość logiczna, mówiąca czy nastąpiła zmiana.\\

Równie ważną funkcją w algorytmie jest funkcja scalająca wierzchołki znajdujące się w jednej społeczności - $merge\_ communities$. Tworzy ona nowy graf, w którym każdej społeczności odpowiada wierzchołek umieszczony w tej społeczności. Suma wag krawędzi między wierzchołkami wewnątrz społeczności staje się pętlą w nowym grafie, a suma krawędzi między społecznościami wagą krawędzi pomiędzy odpowiadającymi im wierzchołkami.\\

Cały algorytm wykonywany jest w funkcji $louvain\_ algorithm$. Wyznaczana jest w niej wartość $\Sigma_{tot}$ dla każdej społeczności, tworzona jest kopia grafu oraz każdy wierzchołek umieszczany jest w swojej społeczności. Następnie w pętli wykonywana jest funkcja $louvain\_ iteration$, tak długo jak powoduje ona jakieś zmiany społeczności. Po każdej iteracji społeczności są scalane. Wynikiem algorytmu jest tablica przynależności. Algorytm nie modyfikuje oryginalnego grafu.

\subsubsection{Implementacja dla nachodzących społeczności}
Algorytm dla społeczności nachodzących wykorzystuje algorytm dla społeczności rozłącznych. Został przede wszystkim zaimplementowany do współpracy z algorytmem Louvaina, ale może działać też z innym niedeterministycznymi algorytmami. Muszą one jednak zwracać tablicę przynależności.\\

Celem algorytmu jest skonstruowanie macierzy przynależności. Jest to macierz $n \times k$, gdzie $n$ to ilość wierzchołków, a $k$ to ilość społeczności. Element $[i][j]$ oznacza jak bardzo wierzchołek i należy do społeczności j.\\

Algorytm rozpoczyna się od pojedynczego przebiegu algorytmu dla społeczności rozłącznych. Jego wynikowa tablica jest tłumaczona tak aby numery społeczności były liczbami od $0$ do $k-1$ ($k$ -ilość społeczności w wyznaczonym podziale). Będzie ona podstawą do wygenerowania macierzy przynależności. Jest ona od razu aktualizowana. Do elementu $[i][j]$ dodajemy ilość połączeń wierzchołka $i$ z wierzchołkami ze społeczności $j$. Następnie wywołujemy algorytm dla społeczności rozłącznych określoną liczbę razy. Po każdym przebiegu wynikowa tablica jest tłumaczona, tak by społeczności odpowiadające tym w tablicy z oryginalnego przebiegu miały ten sam numer. W tym celu dla każdej społeczności znajdowana jest inna, w której ilość pokrywających się wierzchołków jest największa. Następnie macierz przynależności jest aktualizowana, analogicznie jak po początkowym przebiegu.\\ 

Po wykonaniu założonych przebiegów, macierz przynależności jest normalizowana (tak by suma współczynników dla każdego wierzchołka wynosiła 1). Następnie każdemu wierzchołkowi przydzielane są społeczności, dla których współczynnik przynależności przekroczył wartość progową (obliczaną na podstawie parametru funkcji). Współczynniki dla społeczności, które nie przekroczyły wartości progowej są zerowane, a macierz znów jest normalizowana. Jeżeli wierzchołek dla wszystkich społeczności ma współczynnik 0 (tzn. jest wyizolowany), to jest on przydzielany do swojej społeczności. Jeżeli dla wierzchołka, żaden ze współczynników nie przekroczy wartości progowej, to umieszczany jest on w społeczności z największym współczynnikiem. Jest to rozwiązanie na swój sposób prowizoryczne. Takie wierzchołki mogą być bowiem czasami łącznikami pomiędzy społecznościami i w takich sytuacjach wypadałoby umieścić je we wszystkich społecznościach. Jest to jednak problem, który nie został poruszony w tej implementacji i może być obiektem usprawnień w przyszłości.

\subsubsection{Podjęte decyzje}
Metoda dla społeczności nachodzących opisana w artykule 'A Fast Algorithm for Overlapping Community Detection' \cite{pw-paper3} zakładała obecność jeszcze jednego kroku, którego celem miała być ogólna poprawa jakości wyniku. Metoda ta opierała się jednak na modularności Nicosii \cite{pw-paper2}, której obliczenie jest na tyle kosztowne, że zwiększa złożoność algorytmu do $n^3$. Z racji, że zdecydowanie pogorszyło by to czas obliczeń i negatywnie wpłynęło na możliwość wielokrotnego testowania (zwłaszcza dla grafów o kilku tysiącach wierzchołków), a także biorąc pod uwagę fakt, że wyniki bez tego kroku były zadowalające, postanowiliśmy z niego zrezygnować.\\

Dodatkową modyfikacją jest możliwość przekazywania  opcjonalnego parametru do wywołania funkcji\\$louvain\_ algorithm$. Parametr ten wpływa na to jak bardzo element $\frac{\Sigma_{tot}*k_i}{2m^2}$ wpływa na zmianę modularności. Dla wartości mniejszych promuje on powstawanie niewielu rozległych społeczności, a dla większych - powstawanie wielu drobniejszych społeczności. Najlepsze wartości modularności uzyskiwane są co prawda dla wartości 1, ale parametr pozwala na dostosowywanie wyniku do naszych oczekiwań, przy nieznacznych stratach na jakości.

%-----------------------------------------------------%
%Dane testowe
%-----------------------------------------------------%
\section{Dane testowe}
Rozpoczynając pracę, zakładaliśmy, że opisane algorytmy będziemy badać na źródłach danych udostępnianych przez firmę Facebook. Niestety modyfikacje, które chcieliśmy zastosować, nie mogły zostać zrealizowane na zbiorach dotyczących użytkowników tego portalu. Żaden z szeroko dostępnych zestawów danych nie zawierał informacji na temat liczby wspólnych znajomych, a także daty zawarcia znajomości. O ile zdobycie pierwszej wiadomości jest bardzo proste przy użyciu nawet najbardziej naiwnego algorytmu brute force, tak uzyskanie dat można było przeprowadzić tylko w sposób losowy, co nie umożliwiłoby sprawdzenia poprawności założonych usprawnień. Wobec tego swe spojrzenie skierowaliśmy w stronę zbiorów, które na pierwszy rzut oka nie kojarzą się ze społecznościami tak dobrze jak Facebook.
%-----------------------------------------------------%
%NBA
%-----------------------------------------------------%
\subsection{NBA}
'I love this game' - to slogan reklamowy towarzyszący najlepszej lidze koszykarskiej, który odnosi się także do jednego z autorów tej pracy. Wybór tego zbioru danych motywowany był prywatnymi zainteresowaniami autorów, które wiążą się także z ogólną wiedzą na temat samej ligi. Dzięki temu zyskaliśmy możliwość oceny osiąganych rezultatów 'ludzkim okiem', przez co można było je poprawiać i oceniać dzięki swego rodzaju eksperckiej wiedzy na temat NBA.\\

Swoją rolę odegrała tu także ogólna dostępność danych. Ich źródło stanowił portal basketball-reference.com, który stanowi internetową, koszykarską encyklopedię. Korzystając ze scrapera napisanego w języku JavaScript można było zautomatyzować proces wyszukiwania danych. Pomógł tutaj alfabetyczny spis koszykarzy - w ten sposób zdobyliśmy informację na temat każdego z nich, a także w dość łatwo uzyskaliśmy liczbę wszystkich sezonów i klubów, w jakich w swojej karierze występował dany zawodnik. Następnie dane te zostały przetworzone z wykorzystaniem SQL, w celu uzyskania relacji między zawodnikami, a także daty zawarcia znajomości. Liczba wspólnych znajomych została określona przy pomocy prostego algorytmu napisanego w języku C\#.\\

Graf, który uzyskaliśmy, składa się z 4800 wierzchołków. Każdy wierzchołek można przypisać zawodnikowi, który postawił swą stopę na parkietach NBA w ciągu ponad 70 lat istnienia ligi. Dwaj zawodnicy są ze sobą w relacji, jeśli kiedykolwiek zagrali ze sobą w jednym klubie. Data zawarcia znajomości to w tym wypadku data pierwszego meczu, w którym obaj zawodnicy mieli okazję ze sobą zagrać. Tworząc relacje w ten sposób uzyskaliśmy ponad 146 tysięcy krawędzi.

%-----------------------------------------------------%
%Filmweb
%-----------------------------------------------------%
\subsection{Filmweb}
Poszukując parametru daty zawarcia znajomości zwróciliśmy się także ku innej części branży rozrywkowej. Źródło danych w tym wypadku stanowił portal filmweb.pl, który dysponuje ogromną bazą danych na temat aktorów i filmów. Skala tych danych jest tak duża, iż musieliśmy ograniczyć nasze prace tylko do polskich aktorów. W ten sposób uzyskaliśmy graf z 10 000 wierzchołków i 190031 krawędziami.\\

Sposób uzyskania danych był analogiczny do tego prezentowanego w przypadku grafu NBA. Skonstruowany w języku JavaScript scraper został wykorzystany do przejrzenia listy wszystkich polskich aktorów, by następnie dla każdego z nich znaleźć listę kolegów z branży, z którymi najczęściej występował w jednym filmie. Ten zbiór uczyniliśmy znajomymi badanego aktora. Datę zawarcia znajomości w tym wypadku stanowiła data premiery pierwszego filmu/serialu, w którym dana dwójka ze sobą wystąpiła. Dodatkowo na prośbę prowadzącego zbiór danych uzupełniony został o liczbę filmów, w których dana dwójka wspólnie występowała. Informacja ta okazała się również przydatna w modyfikacji jednego z algorytmów.\\

Podsumowując, wierzchołki uzyskanego grafu stanowią polscy aktorzy, pobrani z ogólnodostępnej bazy filmweb. Dwaj aktorzy są ze sobą w relacji, jeśli jeden z nich znajduje się w zakładce 'najczęściej występował z ...' u drugiego. Motywację wyboru tego grafu jako testowego stanowiło ogólne zainteresowanie branżą filmową autorów pracy, a także ogólna dostępność danych. Sama etykietyzacja wierzchołków pozwoliła nam łatwiej oceniać poprawność działania algorytmów - zwłaszcza w początkowej fazie implementacji z wykorzystaniem tylko małych zbiorów danych.

%-----------------------------------------------------%
%Github
%-----------------------------------------------------%
\subsection{GitHub}
W dzisiejszych czasach GitHub jest narzędziem używanym przez niemal
wszystkich ludzi, którzy rozpoczęli swoją przygodę z programowaniem. Nie tylko ułatwia on pracę nad projektem w wieloosobowych zespołach, jednak potrafi być również wizytówką użytkownika. Okazało się, że może też stanowić źródło bardzo ciekawych danych. Portal ten udostępnia informacje: kto z kim pracował przy publicznych projektach, ile kontrybucji wykonał dany użytkownik, kiedy repozytorium zostało ostatnio zmodyfikowane. Dane te są łatwo dostępne za pośrednictwem API. Niestety, jest on zabezpieczone przed atakami typu DDOS poprzez limit zapytań na godzinę, przez co największy z otrzymanych grafów ma 1391 wierzchołków i 21695 krawędzi.\\

Do uzyskania grafu napisany został skrypt w języku Python, który realizuje przeszukiwanie portalu wszerz. Przyjmuje on początkowego użytkownika, a następnie w pętli dodaje na stos wszystkich innych użytkowników, z którymi wybrana osoba współpracowała przy danym projekcie. W następnej kolejności dodaje krawędź między rozpatrywaną osobą, a użytkownikiem i rozpatruje kolejną osobę ze stosu.\\

Podsumowując, graf składa się z wierzchołków, którymi są użytkownicy GitHuba oraz krawędzi znajdujących się między tymi użytkownikami, którzy współpracowali ze sobą w publicznym repozytorium. Warty odnotowania jest fakt, że do krawędzi dodano parametry -datę ostatniej edycji repozytorium oraz liczbę wspólnych commitów dla obu użytkowników.
%-----------------------------------------------------%
%Wyniki
%-----------------------------------------------------%
\section{Wyniki}

%-----------------------------------------------------%
%Girvan Newman
%-----------------------------------------------------%
\subsection{Algorytm Girvan-Newmana}
\subsubsection{Analiza czasowa}
Algorytm Girvan-Newman jest algorytmem o dużej złożoności, co bezpośrednio przekłada się na czas jego działania. Założenie polegające na tym, iż w każdym kroku liczone są najkrótsze ścieżki pomiędzy każdą parą wierzchołków, okazało się kluczowe, jeśli patrzymy pod kątem czasu wykonania. Chcąc szybko osiągnąć wynik, wybór algorytmu Girvan-Newman nie jest wyborem najlepszym już dla społeczności rozłącznych. Rozszerzając algorytm na społeczności nachodzące (przy pomocy algorytmu CONGA z własnymi modyfikacjami) jeszcze bardziej utrudniamy pracę algorytmowi, tworząc kopie wierzchołków i rozszerzając przez to zbiór par, dla których liczone są najkrótsze ścieżki. Doskonale obrazuje to poniższy wykres.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-8.png}
\caption{Wykres czasu w zależności od liczby wierzchołków grafu}
\end{figure}

Dla grafów o liczbie wierzchołków mniejszej od 200 różnica czasu wykonania nie jest zbyt duża, bez znaczenia czy mówimy o klasycznym algorytmie Girvan-Newmana, czy którejś z modyfikacji algorytmu CONGA. Dla większych grafów różnice robią się już jednak o wiele bardziej zauważalne. Już dla grafu o liczbie wierzchołków równej 400 algorytm CONGA jest ponad 35 razy wolniejszy niż metoda Girvan-Newman. Jest to też ostatni graf, na którym udało się przebadać każdą z modyfikacji. Co prawda metoda Girvan-Newman (która przypomnijmy tylko usuwa krawędzie, nie tworząc kopii wierzchołka) poradziła sobie jeszcze z grafami o liczbie 500 i 1700 wierzchołków, to modyfikacja dla społeczności nachodzących nie zmieściła się w limitach czasowych określonych na testy. Warto też zaznaczyć, że w przypadku grafu o liczbie wierzchołków wynoszącej ponad 1700 czas wykonania metody Girvan-Newmana wyniósł niemal 25000 sekund! Jednocześnie na wykresie widać, że czasy działania obu modyfikacji są do siebie bardzo zbliżone, na co wskazuje wzajemne nakładanie się punktów. \\

Dlaczego jednak algorytm CONGA nie poradził sobie z grafem mającym tylko 500 wierzchołków? W dodatku nie udało mu się zakończyć obliczeń także dla grafu mającego 370 wierzchołków. Fakt ten wynika z gęstości grafów, które zostały przebadane. W przypadku grafów gęstych parametr vertex betweennees znacznie częściej osiąga wartość większą aniżeli edge betweenness. Przez to czas wykonania algorytmu rośnie, gdyż powstają coraz to nowe wierzchołki, które są analizowane przy obliczaniu wszystkich najkrótszych ścieżek. O ile w metodzie Girvan-Newman czas wykonania pojedynczej iteracji maleje z każdą kolejną, tak dla CONGA nie mamy takiej gwarancji. Można się wręcz spodziewać, że na początku działania algorytmu, gdzie dochodzi do największej liczby kopii, efekt będzie zupełnie odwrotny. \\

Reasumując, złożoność czasowa algorytmu CONGA uniemożliwia przebadanie go na bardzo dużych grafach. Jest to duży minus tego algorytmu, skutecznie uniemożliwiający wykorzystanie go na dużych zbiorach, gdyż takowe zaczynają się dla tej metody już w granicach 400 wierzchołków. Warto zwrócić również w tym wypadku uwagę na fakt gęstości grafu - na grafie gęstym algorytm może okazać się nie do wykonania w rozsądnym czasie już dla 370 wierzchołków. Jeżeli więc czas działania jest istotny dla odbiorcy (a w większości przypadków tak na pewno jest), algorytm CONGA nie sprawdzi się najlepiej. Jest on natomiast bardzo dobrą metodą szkoleniową do wykorzystania i analizy dla mniejszych i średnich grafów. \\

\subsubsection{Analiza jakościowa}
Po wykonaniu analizy algorytmu, a przed przystąpieniem do implementacji, fakt jego wolnego działania nie został przez nikogo przyjęty ze zdziwieniem. Cechą, którą miał obronić się ten algorytm, była jakość rozwiązania. Rzeczywiście na tym polu można zauważyć, że algorytm zachowuje się o wiele lepiej z perspektywy użytkownika. Po wykonaniu pełnego przebiegu dla danego grafu i otrzymania wynikowego dendrogramu manipulacja wynikiem, którego poszukujemy, zależy już tylko od indywidualnych preferencji użytkownika.\\

Usystematyzować te preferencje pozwoliły metody wykorzystane w implementacji. Pierwszą z nich było określenie, którego z maksimów lokalnych poszukujemy. Na podstawie ich zbioru umożliwiony został wybór, który kwantyl wśród wyników, jest tym nas interesującym. Jeżeli otrzymany wynik nie był jednak zadowalający (np. ze względu na zbyt duży bądź zbyt mały rozmiar społeczności wynikowych), można było zmienić wartość kwantylu i wedle preferencji uzyskać podział na większe bądź mniejsze grupy.\\

Aby zaprezentować wyżej opisane własności najlepiej będzie posłużyć się przykładem. W tym celu skorzystam z grafu uczestników NBA All-Star Games w 4 ostatnich sezonach.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-1.png}
\caption{Największa modularność}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-2.png}
\caption{Modularność na poziomie 0.3 wśród maksimów}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-3.png}
\caption{Najmniejsza modularność pośród maksimów}
\end{figure}

Powyższe przykłady idealnie obrazują, jak zmienia się rezultat w zależności od maksimum lokalnego, które wybierzemy. Widać jednocześnie, że dla najwyższej wartości modularności uzyskano grupy największe pod względem średniej liczby członków. Gdyby jednak poszukiwane grupy okazały się zbyt duże, manipulacja kwantylem pozwoli zmniejszyć ich rozmiar. I tak dla najmniejszego z maksimów graf wynikowy wskaże nam małe grupy, które jednak są najbardziej ściśle z sobą związane. \\

Jest to moment, w którym możemy wskazać duży atut algorytmu CONGA. Właśnie poprzez manipulację współczynnikiem kwantylu możemy w łatwy sposób manipulować wynikiem, jaki jest według nas najlepszy. Posiadając wynikowy dendrogram operacja uzyskiwania grafu wynikowego jest operacją liniową. Dzięki temu możemy odzyskać graf z każdego z kroków algorytmu w bardzo prosty sposób. \\

Warto zaznaczyć, że podczas prezentacji wyników skorzystano z własności maksimum lokalnego. Nic jednak nie stoi na przeszkodzie, aby odzyskać graf wynikowe w mniej zaawansowany sposób. Jednym z nich może być po prostu wybór numeru jednej z iteracji. W dendrogramie podczas każdego kroku rozmiary społeczności wynikowych trzymają się tendencji malejącej. Tak więc im dalszy krok algorytmu wybierzemy, tym graf wynikowy będzie posiadał społeczności mniejszych rozmiarów. \\

Wielkość grup można określić poprzez parametr modularności (tutaj jednak najwyższa wartość nie jest jednak ściśle związana z rozmiarem grupy), natomiast skalę nakładania się społeczności można modyfikować poprzez wykonanie preprocessingu (ta operacja jest jednak bardzo kosztowna, ponieważ de facto dwa razy wykonujemy sam algorytm, jedynie ograniczając liczbę iteracji podczas drugiego wykonania). Sam preprocessingu można również przeprowadzić 'samodzielnie' poprzez określenie pewnej wartości priorytetu, jaki usuwanie krawędzi ma nad kopiowaniem wierzchołka. Wariant ten prezentuje poniższy przykład.\\

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-4.png}
\caption{Wynik przy gap=5 (największa wartość modularności)}
\end{figure}

Graf przedstawiony powyżej został wygenerowany z tego samego grafu początkowego, który analizowany był na poprzednich rysunkach. Prezentowany wynik to wynik dla iteracji o największej wartości modularności. Można zauważyć, iż rezultat ten jest zbliżony do grafu, który został uzyskany przy kwantylu 0.3 dla poprzedniego sposobu wyznaczania wyniku. Parametr gap ogranicza w naszym przypadku liczbę społeczności nachodzących, jednak zachwiana jest w ten sposób tendencja parametru modularności. Przy zerowej wartości priorytetu modularność dla kolejnych iteracji maleje, tutaj tracimy tę własność. Spójrzmy na wynik uzyskany dla kwantylu 0.3.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-5.png}
\caption{Wynik przy gap=5 i kwantylu 0.3}
\end{figure}

Widać więc, że dla prezentowanego maksimum społeczności są większych rozmiarów aniżeli dla wyniku osiągniętego przy największej modularności. W przypadku tak małego grafu ciężko zauważyć różnice na poziomie średniej liczby społeczności dla danego wierzchołka. Przy większych grafach zachowanie to jest jednak zauważalne. \\

To prowadzi do kolejnej obserwacji poczynionej podczas badania algorytmu. Początkową fazę jego działania w głównej mierze stanowi kopiowanie wierzchołka. Jest to oczywiście zależne od wielkości grafu, a także jego gęstości, jednak wszędzie na początku działania algorytmu dokonywania jest seria kopii wierzchołka. Jednocześnie różnica pomiędzy wartościami edge betweenness i vertex betweenness maleje z wykonaniem każdej kolejnej kopii aż do wykonania pierwszego usunięcia krawędzi. Samo usuwanie krawędzi miało odbywać się w jednej iteracji dla wszystkich tych, które mają największy parametr edge betweenness. W rzeczywistości krok te sprowadzał się niemal dla każdej iteracji usunięciem tylko jednej krawędzi, ponieważ przy dużych grafach parametr edge betweenness był bardzo zróżnicowany dla każdej z krawędzi. Dopiero przy ostatnich iteracjach krawędzie usuwane były grupowo (zazwyczaj były to bardzo liczne grupy). \\

Wracając do przykładu, wywołanie algorytmu w ten sposób na pewno przyniesie za sobą korzyść czasową. Jednocześnie jednak należy pamiętać, iż jest to równoznaczne z innym przebiegiem algorytmu. W tym wariancie parametr początkowy wpływa w głównej mierze na liczbe społeczności, do jakiej należy jeden wierzchołek. Im większy priorytet na korzyść parametru edge betweenneess tym mniej podziałów na społeczności nachodzące, a co za tym idzie także większe rozmiary społeczności docelowych.\\

W tym miejscu warto także wspomnieć o rezultatach, jakie udało się osiągnąć, dzięki wprowadzonym modyfikacjom. Posiadanie parametrów dat zawarcia znajomości znacznie ułatwiło proces podziału wierzchołka na oryginał i kopię zarówno od strony implementacyjnej, jak i logiki, która za tym stała. Modyfikacja wyłącznie po latach zawarcia znajomości miała na celu wyłonienie grupy tylko na podstawie daty zawarcia znajomości, a więc opierając się na założeniu, że grupę można identyfikować poprzez bliskie sobie daty zawarcia znajomości wśród jej uczestników. Dodając wagę w postaci liczby wspólnych znajomych przeniesiono nacisk na fakt przebywania dłużej w tej samej grupie - jeśli dwie osoby dłużej przebywają w tym samym otoczeniu, tym ich liczba wspólnych znajomych wzrasta.\\

Analiza obu modyfikacji pod względem parametru modularności nie wskazała znacznych różnic zależnych od modyfikacji. W tym wypadku do analizy wykorzystano wiedzę ekspercką na temat grafu zawodników NBA. Posłużmy się więc przykładem:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-6.png}
\caption{Modyfikacja z rokiem zawarcia znajomości}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/is-sample-7.png}
\caption{Modyfikacja z rokiem zawarcia znajomości i ze wspólną liczbą znajomych}
\end{figure}

Prezentowany przykład skupia się na osobie koszykarza Shaquille O'Neala. Zawodnik ten jest niezwykle utytułowany, a w swej karierze zwiedził mnóstwo klubów. Na wstępie warto zaznaczyć, iż wynik brany tutaj pod uwagę, został wybrany na podstawie największej wartości modularności. Na tym przykładzie możemy zaobserwować, iż przy uwzględnieniu liczby wspólnych znajomych społeczność wynikowa stała się bardzo liczna. Wynika to z faktu, iż podczas swej kariery analizowany zawodnik zdążył zagrać w jednym klubie z wieloma innymi koszykarzami, przez co z wieloma z nich miał dużą liczbę wspólnych znajomych. Gdy ten parametr pominiemy podany zawodnik znajdzie się w 3 społecznościach. Ponadto jedna z nich wyraźnie wskazuje na zespół Los Angeles Lakers, do którego O'Neal dołączył w roku 1996. \\

Podobnych przykładów można znaleźć znacznie więcej w zależności od grafu, który analizujemy. Wskazują one, że dodanie liczby wspólnych znajomych jako wagi rzeczywiście wpływa na rozkład grup. Dysponując wiedzą na temat wierzchołków i realnych zależności można skupić się na analizie pojedynczych wierzchołków, jednak obserwacje w skali makro również przynoszą pewne wnioski. Podchodząc do problemu w ten sposób można zauważyć, iż w przypadku dodania wag krawędzi wynik 'spłaszcza się', zwracając mniej społeczności, które są za to bardziej liczne. Skorzystanie z samej daty zawarcia znajomości dzieli graf na mniejsze społeczności. \\

Warto w tym miejscu wspomnieć jednak o tym, iż uzyskany wynik niekoniecznie jest zobowiązujący. W przypadku uzyskania zbyt dużych społeczności modyfikacja wyboru maksimum lokalnego zgodnie z obserwacjami poczynionymi w poprzednich akapitach. Algorytm CONGA daje bardzo dużą elastyczność w kwestii uzyskanych wyników. Modyfikacje, które zostały wykonane w głównej mierze skupiają się na rezultacie końcowyw. Ten jednak oparty jest na mierze modularności, która nie jest jednak w żaden sposób miarą wiążącą. W przypadku tego algorytmu to użytkownik 'decyduje', jaki podział na grupy według niego będzie najlepszy. Jeżeli oczywiście tylko cierpliwość pozwoli mu na odczekanie końca działania algorytmu.\\

%-----------------------------------------------------%
%LPA
%-----------------------------------------------------%
\subsection{Label Propagation Algorithm}
\subsubsection{Analiza czasowa}
Poniżej przedstawiono wyniki analizy czasowej dla standardowego zbioru przykładów użytego w pracy.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-benchmark.png}
\caption{Wynik analizy czasowej algorytmu Label Propagation}
\end{figure}

Algorytm LPA, zgodnie z oczekiwaniami, okazał się być algorytmem wystarczająco szybkim, aby przeprowadzić obliczenia na wszystkich zbiorach danych w projekcie. Z wyników można odczytać, że implementacja biblioteczna jest nieporównywalnie szybsza od implementacji własnej dla dużych zbiorów danych. Jedną z przyczyn takich wyników, może być fakt, że backend biblioteki igraph jest napisany w języku C, gdzie dostęp do elementów grafu jest dużo szybszy niż przy użyciu dodatkowej warstwy abstrakcji. Innym powodem może być nieświadome użycie w implementacji operacji, która zmusza bibliotekę do przebudowania struktury grafu. Niestety, podczas analizy kodu źródłowego rozwiązania, nie udało się znaleźć źródła problemu.\\

Na powyższym wykresie można zauważyć, że zależność czasu od ilości wierzchołków jest liniowa. Pomiarem, który zdecydowanie odbiegł od oczekiwań, jest pomiar dla największego grafu przy modyfikacji związanej z datami. Prawdopodobna przyczyna wiąże się z tym, że algorytm z modyfikacją podczas rozpatrywania remisu tworzy tablice pomocnicze oraz oblicza dla nich odchylenie standardowe. W przypadku grafu gęstego remisy występują często, co skutecznie spowolniło działanie programu.\\

Jak szybkie potrafią być algorytmy do wykrywania społeczności pokazała implementacja przy użyciu biblioteki CUDA. Rezultat znacząco przekroczył nasze oczekiwania. Co prawda implementacja ta jest częściowo okrojona, lecz z powodzeniem można ją rozszerzać o modyfikacje zaimplementowane w wersji na CPU.

\begin{table}[H]
  \begin{center}
    
    \label{tab:table1}
    \begin{tabular}{c|c|c|c}
      \textbf{$|V|$} & \textbf{$|E|$} & \textbf{Czas alokacji (ms)} & \textbf{Czas obliczeń (ms)}\\
      \hline
      4800 & 146369 & 335 & 6\\
      10000 & 190031 & 225 & 33 \\
      3096951 & 2899051 & 467 & 162 \\
    \end{tabular}
    \caption{Wyniki czasowe algorytmu LPA zaimplementowanego przy użyciu biblioteki CUDA}
  \end{center}
\end{table}

W podanych wynikach nie uwzględniam przebudowy reprezentacji grafu - została ona zaimplementowana naiwnie, ponieważ możemy założyć, że na wejściu otrzymujemy żądaną reprezentację. Uwzględniony został jednak czas alokacji pamięci na karcie graficznej - jest on znacząco większy od czasu obliczeń. Jego znaczenie może zostać zredukowane, gdy przetwarzamy graf kilkukrotnie, wtedy wystarczy nam jedna alokacja, na początku działania programu. Patrząc na wyniki wnioski nasuwają się same - przy sieciach o rozmiarach rzędu milionów wierzchołków i krawędzi, ta implementacja sprawdzi się zdecydowanie najlepiej.

\subsubsection{Analiza jakościowa}

Analizę jakościową rozpoczniemy od porównania z implementacją wbudowaną w bibliotekę igraph. Do analizy użyto grafów z listy wspólnych grafów testowych.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-build-lpa-0.png}
\caption{Wizualizacja wyniku działania algorytmu LPA w bibliotece igraph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-build-lpa-2.png}
\caption{Wizualizacja wyniku działania algorytmu LPA w bibliotece igraph}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-build-nba.png}
\caption{Wizualizacja wyniku działania algorytmu LPA w bibliotece igraph}
\end{figure}

Można zaobserwować, że algorytm przy gęstych grafach tworzy małą liczbę dużych społeczności. We własnej implementacji, przy ograniczeniu liczby iteracji do 6 uzyskujemy rezultat bardziej zrównoważony:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-0.png}
\caption{Wizualizacja wyniku działania własnej implementacji algorytmu LPA} 
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-2.png}
\caption{Wizualizacja wyniku działania własnej implementacji algorytmu LPA}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-nba.png}
\caption{Wizualizacja wyniku działania własnej implementacji algorytmu LPA}
\end{figure}

Poniżej przedstawiono wyniki działania dla modyfikacji związanej z datami zawarcia znajomości. O ile przy drugim grafie wynik nie wyglada lepiej niż w wersji bez modyfikacji, to w przypadku grafu mniejszego można zauważyć zdecydowanie bardziej naturalne rozłożenie etykiet. Również w przypadku grafu największego rozmiary społeczności stały się dużo bardziej zbalansowane - nie pojawiła sie społeczność, która zdecydowanie przeważałaby w grafie. Modyfikacja ta ma znaczący wpływ na wynik działania algorytmu i mogłaby z powodzeniem zostać użyta w praktycznej analizie problemu związanego z sieciami społecznościowymi.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-m1-0.png}
\caption{Wizualizacja wyniku działania modyfikacji związanej z datami algorytmu LPA} 
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-m1-2.png}
\caption{Wizualizacja wyniku działania modyfikacji związanej z datami algorytmu LPA}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-m1-nba.png}
\caption{Wizualizacja wersji dla społeczności nachodzących algorytmu LPA}
\end{figure}

Poniżej przedstawiono wyniki algorytmu w wersji dla społeczności nachodzących.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-ol-0.png}
\caption{Wizualizacja wersji dla społeczności nachodzących algorytmu LPA}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-lpa-ol-2.png}
\caption{Wizualizacja wersji dla społeczności nachodzących algorytmu LPA}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-ol-nba.png}
\caption{Wizualizacja wersji dla społeczności nachodzących algorytmu LPA}
\end{figure}

O ile dla najmniejszego grafu otrzymane rezultaty są zadowalające, tak przy grafie średniej wielkości nie otrzymano oczekiwanych wyników. Nie udało się wyznaczyć społeczności nachodzących, a te, które zostały wskazane, są niewielkie, nie można wyznaczyć między nimi granic. W przypadku społeczności nachodzących nie jest łatwo przeprowadzić analizę na grafie dużym. Warto jednak zauważyć ciekawą zależność - wierzchołki łączące w sobie wiele społeczności utworzyły naturalny szkielet sturktury. Wierzchołki należące do jednej społeczności skupiły się w środku oraz na obwodzie.\\

Ciekawe zjawisko można zaobserwować badając grafy przetwarzane równolegle. Podwójne buforowanie wierzchołków jest wymuszone w implementacji CUDA, aby uniknąć konfliktów w dostępie do pamięci. Na CPU grafy przetwarzane są synchronicznie. Poniżej przedstawiono wizualizację dla pełnego grafu NBA:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ms-cuda.png}
\caption{Wizualizacja wyniku działania LPA CUDA dla grafu NBA}
\end{figure}

Otrzymane rezultaty sa znacząco inne niż w wersji synchronicznej. Społeczności jest zdecydowanie więcej, część z nich 'rozciąga się' przez znaczną część grafu. Małe kliki uformowały społeczności, co jest porządanym rezultatem. Biorąc pod uwagę czas wykonania tej wersji algorytmu wyniki są bardzo zadowalające.

%-----------------------------------------------------%
%OCDLCE
%-----------------------------------------------------%
\subsection{Overlapping Community Detection by Local Community Expansion}
\subsubsection{Analiza czasowa}
Poniżej znajduje się wykres analizy czasowej dla zbioru przykładów użytego w pracy:
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-time.png}
\caption{Wynik analizy czasowej algorytmu OCDLCE}
\end{figure}

Algorytm OCDLCE nie spełnił przewidywanych oczekiwań czasowych i już przy danych rzędu tysiąca jego działanie trwało relatywnie długo. Ciekawą obserwacją, widzoczną na powyższym wykresie, jest fakt, że modyfikacje kroku pierwszego, które powinny być istotnie wolniejsze, nie mają dużego wpływu na rezultat. Modyfikacja modularności również nie wpłynęła istotnie na czas wykonania programu.\\

Z powyższych faktów można wywnioskować, że najbardziej kosztowny jest drugi krok algorytmu. Autorzy pracy naukowej, na której bazuje implementacja przekonywali, że pomimo teoretycznej złożoności kwadratowej, zazwyczaj krok ten wykonuje się dużo szybciej. Istnieje jednak możliwość, że faza łączenia społeczności nie została zaimplementowana w pełni optymalnie, co może mieć wpływ na uzyskany rezultat.\\

Innym z powodu tego typu achowania może być fakt, że w pracy, na której bazowano, zakładano liczbę krawędzi rzędu liczby wierzchołków, a rozpatrywane przez nas grafy były dużo bardziej gęste.\\

\subsubsection{Analiza jakościowa}
W algorytmie przeprowadzono dwie modyfikacje i okazało się, że obie mają istotny wpływ na uzyskany wynik. Wszystkie cztery przetestowane kombinacje zwracają rezultaty zbliżone do oczekiwanych.\\ 

Pierwszą zmianą było użycie innej miary modularności. W tym aspekcie modularność f okazała się o wiele skuteczniejsza. Główną jej zaletą jest elastyczność związana z parametrem alpha. Dla wartości 1 parametru alpha obie wersje zachowują się podobnie. Przetestowano również dwie inne wartości: 0.5 i 1.5. Dla pierwszej z nich liczba znalezionych społeczności w pierwszym kroku okazała się nieznacznie mniejsza, zaś dla drugiej, kilka razy większa. Warto odnotować, że w kroku drugim różnica ta praktycznie zanika. Poniżej podane są przykładowe liczby społeczności po danym kroku dla grafu o 1391 wierzchołkach oraz wizualizację uzyskanych grafów:\\
Dla wersji bez modyfikacji:\\
Liczba społeczności po pierwszym kroku: 101\\
Liczba społeczności po drugim kroku: 87\\
Dla wersji używającej modularności f z parametrem alpha równym 1.5:\\
Liczba społeczności po pierwszym kroku: 163\\
Liczba społeczności po drugim kroku: 88\\

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-1.png}
\caption{Wizualizacja dla wersji bez modyfikacji}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-2.png}
\caption{Wizualizacja dla wersji korzystającej z modularności f}
\end{figure}

Otrzymane rezultaty wyglądają całkiem podobnie, lecz wersja korzystającej z modularności f wydaje się wyznaczać lepsze rezultaty, przy użyciu podobnej liczby społeczności. To, że rezultat jest lepszy, można uzasadnić mniejszą liczbą wierzchołków należących do bardzo dużej liczby społeczności oraz większą ich liczbą należących do kilku społeczności, będących ich nakładającą się częścią.\\

Kolejną wprowadzoną modyfikacją była zmiana kroku pierwszego, polegającego na wyznaczaniu początkowych społeczności. Rezultaty okazały się znacznie odległe od oczekiwanych. Pierwszą intuicją związaną z ową modyfikacją było zmniejszenie liczby wierzchołków, gdyż znalezienie trzech wierzchołków, niebędących aktualnie parami ze sobą w żadnej społeczności, wydaje się trudniejsze od znalezienia pary o tej własności. Okazało się jednak, że liczba społeczności wzrosła kilkukrotnie. Po dłuższym zastanowieniu można dojść do wniosku, że może wynikać z faktu rozpoczęcia analizy z poziomu 3 wierzchołków, a nie z 2. W takim wypadku początkowa modularność może być mniejsza. Kolejnym, niezbyt pożądanym efektem, jest izolacja wierzchołków o małej liczbie sąsiadów. W związku z powyższym uzyskuje się graf o wielu początkowych społecznościach, względnie dużej liczbie wierzchołków mających przypisanych wiele społeczności oraz niepomijalnej liczbie wierzchołków, które nie są przypisane do żadnej ze społeczności. Poniżej znajdują się wyniki owej modyfikacji dla już wcześniej analizowanego grafu.\\
Dla zmodyfikowanej wersji z podstawową modularnością:\\
Liczba społeczności po pierwszym kroku: 307\\
Liczba społeczności po drugim kroku: 95\\
Dla zmodyfikowanej wersji używającej modularności f z parametrem alpha równym 1.5:\\
Liczba społeczności po pierwszym kroku: 380\\
Liczba społeczności po drugim kroku: 108\\


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-3.png}
\caption{Wizualizacja dla zmodyfikowanej wersji z podstawową modularnością}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-4.png}
\caption{Wizualizacja dla zmodyfikowanej wersji korzystającej z modularności f}
\end{figure}

Jak widać na powyższych wizualizacjach, rezultaty nie wydają się tak dobre, jak dla podstawowej wersji algorytmu. Wersja korzystające z domyślnej modularności nie przypisała kilku wierzchołkom żadnej społeczności, zaś wersja z modularnością f  przypisała do niektórych wierzchołków ogromną liczbę społeczności.\\

Jednak dla pewnych grafów, chociażby takich o bardzo małej liczbie wierzchołków, owa modyfikacja, a właściwie modyfikacja korzystająca z modularności f zwraca pozytywne, oczekiwane na początku rezultaty. Poniżej znajduje się zestawienie wszystkich czterech konfiguracji dla pewnego grafu o 83 wierzchołkach:
Dla wersji bez modyfikacji:\\
Liczba społeczności po pierwszym kroku: 25\\
Liczba społeczności po drugim kroku: 22\\
Dla wersji używającej modularności f z parametrem alpha równym 1.5:\\
Liczba społeczności po pierwszym kroku: 24\\
Liczba społeczności po drugim kroku: 20\\
Dla zmodyfikowanej wersji z podstawową modularnością:\\
Liczba społeczności po pierwszym kroku: 8\\
Liczba społeczności po drugim kroku: 7\\
Dla zmodyfikowanej wersji używającej modularności f z parametrem alpha równym 1.5:\\
Liczba społeczności po pierwszym kroku: 8\\
Liczba społeczności po drugim kroku: 7\\

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-5.png}
\caption{Wizualizacja dla wersji z podstawową modularnością}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-6.png}
\caption{Wizualizacja dla wersji korzystającej z modularności f}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-7.png}
\caption{Wizualizacja dla zmodyfikowanej wersji z podstawową modularnością}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/bt-sample-8.png}
\caption{Wizualizacja dla zmodyfikowanej wersji korzystającej z modularności f}
\end{figure}

Zmodyfikowana wersja z użyciem domyślnej modularności co prawda zostawia jeden wierzchołek bez przypisanej społeczności, ale po użyciu modularności f uzyskano najbardziej zrównoważony i bardzo zbliżony do początkowych oczekiwań rezultat.\\

Podsumowując, zdecydowanie warto używać modularności f zamiast modularności M, a w niektórych przypadkach można rozważyć skorzystanie z modyfikacji pierwszego kroku algorytmu.
%-----------------------------------------------------%
%Louvain
%-----------------------------------------------------%
\subsection{Algorytm Louvain}
\subsubsection{Analiza czasowa}
Złożoność czasowa, podobnie jak w przypadku pozostałych algorytmów, została przeanalizowana za pomocą standardowego zbioru przykładów.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/pw-louvein-time.png}
\caption{Wynik analizy czasowej algorytmu Louvain}
\end{figure}

Podobnie jak w przypadku algorytmu LPA implementacja algorytmu z biblioteki igraph osiąga zdecydowanie lepsze czasy. Jednak wciąż algorytm zaimplementowany przez nas jest w stanie w sensownym czasie zakończyć działanie nawet dla grafu o liczbie wierzchołków rzędu kilku tysięcy. Wykres wskazuje co prawda, że  że niestety nie udało się zachować złożoności $n\log{n}^2$. Powodem była konieczność przeliczania parametru $k_{in}$. Podjęte zostały  próby przerobienia algorytmu, by wartości nie były wyliczane od nowa w każdej iteracji, a jedynie uaktualniane, niestety okazały się one bezowocne. W przypadku wywołań wykorzystujących zmodyfikowaną wartość parametru $param$ możemy zauważyć, że wywołania promujące mniejszą ilość społeczności są nieznacznie szybsze (ok 50s dla 1768 wierzchołków, w porównaniu do ok 53s). Jest to spowodowane tym, że w każdym kroku bardziej zmniejsza się ilość wierzchołków. Analogicznie wywołanie promujące mniejsze społeczności trwa nieznacznie dłużej (ok 56s dla 1768 wierzchołków).\\

Wersja dla społeczności nachodzących wypada dość podobnie do standardowej, gdyż jest to kilkukrotne wywołanie standardowej wersji wraz z analizą wyniku.

\subsubsection{Analiza jakościowa}
	Analiza jakościowa wyników zaimplementowanego algorytmu wypada z kolei zdecydowanie bardziej optymistycznie. Została przeprowadzona na  podgrafach grafu przedstawiającego połączenia pomiędzy zawodnikami NBA (wszystkie podgrafy wykorzystują ilość lat od pierwszego wspólnego wystąpienia jako wagę krawędzi). \\

Pierwszym  był podgraf składający się z zawodników grających w drużynach All-Star z ostatnich 4 lat:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/pw_img1.png}
\caption{Wizualizacja wyniku działania algorytmu}
\end{figure}

W celu dokładnego zbadania jakości wyniku posłużyliśmy się wartością modularności ($Q= \frac{1}{2m} \sum\limits_{ij} [A_{ij} - \frac{d_i d_j}{2m}]\delta(s_i,s_j)$). Dla 5 różnych przebiegów (pamiętając, że algorytm jest niedeterministyczny) osiągnęliśmy następujące wartości: 0.63909, 0.63779, 0.63779, 0.63909, 0.63236. W porównaniu z wartością modularności, którą osiąga algorytm z biblioteki igraph: 0.59064, możemy stwierdzić że wyniki naszego algorytmu dla małych grafów (do 100 wierzchołków) wypadają bardzo dobrze. Ponadto, pomimo niedeterminizmu, są dosyć porównywalne. Dla potwierdzenia spójrzmy jeszcze na wartości dla podgrafu składającego się z zawodników z ostatnich 10 drużyn All-Star: 0.42936, 0.41937, 0.44528, 0.44048, 0.42730, 0.43560 i wynik  dla algorytmu bibliotecznego: 0.40900.\\\\ 

Rozważmy więc graf o większej ilości wierzchołków. Grafem, który będziemy rozważać, będzie podgraf składający się z 500 zawodników NBA:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/pw_img2.png}
\caption{Wizualizacja fragmentu wyniku działania algorytmu dla grafu o 500 wierzchołkach}
\end{figure}

W celu głębszej analizy spójrzmy na odchylenie standardowe modularności obliczonych dla 15 wywołań algorytmu: 0.0186 oraz wartość średnią tych modularności: 0.39502. Porównując to z wartością uzyskaną przez algorytm z bibliotek igraph: 0.44497 zauważamy, że dla większych grafów (a przynajmniej dla tego podgrafu) nasz algorytm nieznacznie przegrywa z bibliotecznym. Patrząc jednak na wartość największą z 15 przebiegów: 0.41930 różnica nie jest tak duża.\\\\

Ostatnim grafem, który rozpatrzymy w przypadku społeczności rozłącznych będzie pełen graf NBA. 	

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/pw_img3.png}
\caption{Wizualizacja wyniku działania algorytmu dla grafu NBA}
\end{figure}

Wynik algorytmu jest bardzo trudny, a nawet niemożliwy, do analizy na podstawie wizualizacji, więc spójrzmy na wartość modularności: dla naszej implementacji wynosi ona 0.57375, a dla algorytmu bibliotecznego: 0.58668. Wartość jest więc nieznacznie niższa, ale dalej bliska tej od sprawdzonej implementacji.
\\ Analiza jakościowa wykazała, że algorytm Louvain dla społeczności rozłącznych w naszym wykonaniu daje poprawne wyniki, zarówno dla małych jak i dla dużych grafów.
\\ Spójrzmy jeszcze na wynik algorytmu dla społeczności nachodzących. W tym wypadku znowu posłużymy się grafem 500 graczy.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/pw_img4.png}
\caption{Fragment wizualizacji dla społeczności nachodzących}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/pw_img5.png}
\caption{Fragment wizualizacji dla społeczności nachodzących}
\end{figure}
Obserwując strukturę grafu, zwłaszcza na przykładzie społeczności o numerach 1 i 2 można zakładać, że jest to dość sensowny wynik. Wskazuje też na to wartość modularności Nikozji która wynosi 1.22.
%-----------------------------------------------------%
%Porównanie
%-----------------------------------------------------%
\subsection{Porównanie}
Metody prezentowane w naszym raporcie znacznie różnią się od siebie pod kątem implementacji. Pierwsze różnice można zaobserwować już w sposobie podejścia do problemu. Metoda Girvan-Newmana, jak i jej rozszerzenie w postaci algorytmu CONGA, jako jedyna rozpoczynała analizę problemu od niezmodyfikowanego grafu początkowego. Pozostałe metody analizę problemu rozpoczynały od pojedynczych wierzchołków, łącząc je w większe społeczności.\\

Wyżej wymieniona zależność niesie za sobą różnice czasowe w wykonaniu każdej z metod. Algorytm CONGA okazał się zdecydowanie najwolniejszy ze wszystkich, przy okazji osiągając rezultaty, które skutecznie eliminują go jako metodę użyteczną w analizie problemów związanych z naprawdę dużymi grafami. Na przeciwnym końcu znajduje się algorytm LPA - jego wykonanie jest niezwykle szybkie, zwłaszcza, gdy wzbogacono go o implementację na CUDA. Algorytmy OCDCLE i Louvain również okazały się wystarczająco szybkie, aby myśleć o ich użyciu w kontekście grafów o liczie wierzchołków mniejszej mieszczącej się w granicy 10 000. Z tej dwójki to metoda Louvain znalazła się na drugim miejscu, jeśli chodzi o wyniki czasowe. \\

Warto również zwrócić uwagę na założenia początkowe twórców każdego z algorytmu. Prezentowana praca skupiła się na wyszukiwania społeczności nachodzących, lecz trzy z użytych algorytmów początkowo zostały stworzone w celu wyszukiwania społeczności rozłącznych. Jedynie algorytm OCDCLE był bezpośrednio zwrócony na społeczności nachodzące. W związku z tym faktem usprawnienia związane z tym algorytmem skupiły się głównie na poprawie wyniku, natomiast przy pozostałych metodach własne pomysły można było skierować na przejście pomiędzy algorytmem dla społeczności rozłącznych i jego wersją dla społeczności nachodzących. Warto również zwrócić uwagę na to, iż metoda Louvain została wybrana jako metoda eksperymentalna. Wzmianki o niej można znaleźć w nielicznych dokumentach, a sam wybór stanowił chęć sprawdzenia działania tej metody. Po drugiej stronie można tutaj umieścić metodę Girvan-Newmana, która pojawia się niemal w każdej pracy związanej ze społecznościami nachodzącymi. \\

Ciężko obiektywnie określić jakość rozwiązań, jakie zwracane są przez każdą z metod. Warto jednak nadmienić, iż to, na ile wynik jest dobry, może zależeć subiektywnie od danego użytkownika. Jednemu z odbiorców jako wynik docelowy może odpowiadać trzy duże grupy wynikowe, innemu duża liczba małych, aczkolwiek mocno związanych grupek. Warto więc porównać metodami pod kątem manipulacji osiąganymi rezultatami. Jako wzorzec przyjąć można tutaj metodę Girvan-Newmana, która umożliwia odzyskanie wyniku po każdej z wykonanych iteracji. Ze względu na wyniki czasowe jej przydatność jest jednak wątpliwa. Dla porównania algorytm LPA zwraca jednoznaczny wynik, jednak czas jego wykonania pozwala na wielokrotne uruchomienie algorytmu z różnymi parametrami, co powoduje zwracanie różnych wyników. Sposób ten okazuje się o wiele szybszy nawet przy wielokrotnym wywołaniu. Podobnie z problemem różnych rezultatów radzą sobie także algorytmy OCDCLE i Louvain. Ich czasy wykonania pozwalają na manipulację parametrami i zwracanie różnych wyników. Ponadto algorytm OCDCLE, dzięki temu, że od początku był tworzony z myślą o społecznościach nachodzących, posiada specjalny parametr regulujący stopień nachodzenia się społeczności. Natomiast w przypadku metody Louvain warto odnotować, iż osiągnięte rezultaty okazały się lepsze od tych osiągniętych w oryginalnej pracy.\\

Porównując algorytmy pod względem ich słabych stron, możemy stwierdzić, iż metoda LPA radzi sobie słabo z dużymi grafami, mając tendencje do tworzenia bardzo licznych społeczności. Metoda OCDCLE posiada natomiast tendencję do tworzenia jednowierzchołkowych społeczności. Metoda Girvan-Newmana zdaje się omijać te błędy, jednak jej złożoność czasowa okazuje się tutaj barierą nie do pokonania. Problemem w algorytmie Louvain jest niedeterminizm, który może powodować pewne zaburzenia w wynikach dla niektórych z badanych grafów. \\
 
%-----------------------------------------------------%
%Wnioski
%-----------------------------------------------------%
\subsection{Wnioski}
Kończąc poprzedni akapit negatywnymi aspektami algorytmów, ten można rozpocząć od pozytywnej wiadomości - wykrywanie społeczności nachodzących jest to temat niezwykle ciekawy, który autorów niniejszej pracy zaintrygował swą złożonością, liczbą różnych możliwych podejść, a także szeroką gamą modyfikacji, którą można zastosować w pracy nad zaproponowanymi już przez kogoś metodami. Praca ta przedstawia 4 różne algorytmy, lecz jesteśmy pewni, iż skupiając się na analizie tylko jednego z nich, każdy z autorów mógłby wykonać nie mniej pracy niż przy obecnym projekcie. \\

Temat społeczności nachodzących zachęca do analizy posiada dobrze zbilansowany próg wejścia do problemu. Nie jest to temat do analizy w jeden wieczór, jednak wiedza, jaką trzeba posiadać, aby zacząć analizować i modyfikować algorytmy nie stanowi bariery, której forsowanie zajmuje dużą ilosć czasu. \\

W swej pracy natknęliśmy się także na problem z dostępnością danych do modyfikacji, jakie chcieliśmy zastosować. To jednak zaprowadziło nas do prostego wniosku, iż grafy znajdują się wszędzie wokół nas. W świecie social media i Internetu temat społeczności można utożsamiać z dużą ilością zbiorów danych o przeróżnych cechach i charakterystyce. Nas badania tego typu zaprowadziły w stronę dziedzin prywatnych zaintersowań, a więc koszykówki, filmu i informatyki. \\

Podczas pracy odkryto, iż głównym problemem analizy społeczności nachodzących jest czas wykonania algorytmów. Większość znanych metod potrzebuje znacznej ilości czasu, aby w pełni przeanalizować graf o złożonej strukturze. Brakuje także swego rodzaju miary, która pozwala jasno sklasyfikować graf jako poprawny. Opierać można się tu tylko na intuicji, a także rozmiarze społeczności docelowych, jakie chcemy uzyskać, a także stopniu ich nachodzenia się. \\

Problemem natury programistycznej jest natomiast analiza działania kodu w trakcie implementacji. W naszym przypadku dysponowaliśmy swego rodzaju wiedzą ekspercką na temat grafu NBA, jednak większość ogólnie dostępnych danych to prosty zbiór wierzchołków z etykietą w postaci numerów, a także krawędzi pomiędzy wierzchołkami. W związku z tym grafy przez nas stworzone mogą przysłużyć się przyszłej pracy nad wszelkiego rodzaju problemami dotyczącymi społeczności. \\

Reasumując stwierdzamy, iż temat wyszukiwania społeczności nachodzących jest tematem bardzo ciekawym i cieszymy się z takiego wyboru tematu. Nasza wiedza w zakresie grafów, a także samych grafów społecznościowych wzrosła, a praca nad projektem sprawiła satysfakcję i radość. Nie był to projekt, do którego wracało się niechętnie. Uważamy także, że repozytorium kodu, które stworzyliśmy, może stanowić źródło dla dalszych kontrybucji i liczymy, że kod z tego projektu nie zostanie zamknięty w jednej z 'szuflad' Githuba. \\
\newpage
\printbibliography

\end{document}

